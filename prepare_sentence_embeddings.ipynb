{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "lang = \"CN\"\n",
    "tr = 2\n",
    "lag = 2\n",
    "annotation_path = Path(f\"annotation/{lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(annotation_path / f\"lpp{lang}_word_information.csv\", index_col=0)[[\"word\", \"offset\", \"section\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"time\"] = (df.offset + tr / 2) // tr * tr + tr / 2\n",
    "# res = []\n",
    "# for lag in range(lag + 1):\n",
    "#     res.append(df[df.time >= 0].copy())\n",
    "#     df[\"time\"] = df.time + tr\n",
    "# df = pd.concat(res)\n",
    "# df = df.sort_values(\"offset\").groupby([\"section\", \"time\"]).word.apply(lambda x: x.str.cat(sep=\" \")).str.replace(\"' \", \"'\").reset_index(name=\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = df.offset // tr * tr\n",
    "df.loc[df.time < 0, \"time\"] = 0\n",
    "df = df.sort_values([\"section\", \"time\"]).groupby([\"section\", \"time\"]).word.apply(lambda x: x.str.cat(sep=\" \")).reset_index(name=\"sentence\")\n",
    "df[\"sentence\"] = df.groupby(\"section\").sentence.transform(lambda x: (x + \" \").cumsum().str.strip())\n",
    "df[\"sentence\"] = df.sentence.str.replace(\"' \", \"'\").str.replace(\" s \", \"'s \").str.replace(\" i \", \" I \").str.replace(\" ve \", \"'ve \").str.replace(\" t \", \"'t \")\n",
    "df[\"duration\"] = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "embeddings = []\n",
    "for section in tqdm(df.section.unique()):\n",
    "    sentences = list(df[df.section == section].sentence)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', truncation_side=\"left\")\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2').to(device)\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    tokenizer.model_max_length = 4096\n",
    "    n_tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')[\"attention_mask\"].sum(axis=1)\n",
    "    rolled_n_tokens = np.roll(n_tokens, 1)\n",
    "    rolled_n_tokens[0] = 0\n",
    "    n_tokens -= rolled_n_tokens\n",
    "    n_tokens = n_tokens.to(device)\n",
    "\n",
    "    total_samples = len(encoded_input[\"input_ids\"])\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_batches), leave=False):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, total_samples)\n",
    "            batch_input = {k: v[start_idx:end_idx] for k, v in encoded_input.items()}\n",
    "            model_output = model(**batch_input)\n",
    "            n_ones = batch_input[\"attention_mask\"].sum(axis=1)\n",
    "            n_ones_to_remove = n_ones - n_tokens[start_idx:end_idx]\n",
    "            for i in range(end_idx - start_idx):\n",
    "                batch_input[\"attention_mask\"][i, :n_ones_to_remove[i]] = 0\n",
    "            embeddings.append(mean_pooling(model_output, batch_input[\"attention_mask\"]).cpu().numpy())\n",
    "embeddings = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(embeddings, columns=[f\"sBERT_{i}\" for i in range(embeddings.shape[1])])\n",
    "df = pd.concat([df, embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(annotation_path / f\"lpp{lang}_sentence_embeddings_sBERT.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = df.offset // tr * tr\n",
    "df.loc[df.time < 0, \"time\"] = 0\n",
    "df = df.sort_values([\"section\", \"time\"]).groupby([\"section\", \"time\"]).word.apply(lambda x: x.str.cat(sep=\" \")).reset_index(name=\"sentence\")\n",
    "df[\"sentence\"] = df.groupby(\"section\").sentence.transform(lambda x: (x + \" \").cumsum().str.strip())\n",
    "df[\"sentence\"] = df.sentence.str.replace(\"' \", \"'\").str.replace(\" s \", \"'s \").str.replace(\" i \", \" I \").str.replace(\" ve \", \"'ve \").str.replace(\" t \", \"'t \")\n",
    "df[\"duration\"] = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laser_encoders import LaserEncoderPipeline\n",
    "batch_size = 32\n",
    "lang_corresp = {\"FR\": \"french\", \"EN\": \"english\", \"CN\": \"chinese\"}\n",
    "encoder = LaserEncoderPipeline(lang=\"french\", batch_size=batch_size)\n",
    "results = []\n",
    "for section in tqdm(df.section.unique()):\n",
    "    sentences = df[df.section == section].sentence\n",
    "    embeddings = encoder.encode_sentences(sentences=sentences)\n",
    "    rolled_mask = ~np.roll(embeddings[\"encoder_padding_mask\"], shift=1, axis=0)\n",
    "    rolled_mask[0] = False\n",
    "    embeddings[\"encoder_out\"][rolled_mask] = -np.inf\n",
    "    results.append(embeddings[\"encoder_out\"].max(axis=1))\n",
    "features = [\"LASER_\" + str(i) for i in range(results[0].shape[1])]\n",
    "embeddings = pd.DataFrame(np.concatenate(results), columns=features)\n",
    "df = pd.concat([df, embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(annotation_path / f\"lpp{lang}_sentence_embeddings_LASER.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe = pd.read_parquet(annotation_path / f\"lpp{lang}_word_embeddings_GloVe.parquet\")\n",
    "GloVe[\"word_index\"] = GloVe.index\n",
    "features = [col.replace(\"GloVe\", \"GloVeBag\") for col in GloVe.columns if \"GloVe\" in col]\n",
    "df[\"word_index\"] = df.index\n",
    "df = df.merge(GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = (df.offset + tr / 2) // tr * tr + tr / 2\n",
    "res = []\n",
    "for lag in range(lag + 1):\n",
    "    res.append(df[df.time >= 0].copy())\n",
    "    df[\"time\"] = df.time + tr\n",
    "df = pd.concat(res)\n",
    "df = df.sort_values(\"offset\").groupby([\"section\", \"time\"]).word.apply(lambda x: x.str.cat(sep=\" \")).str.replace(\"' \", \"'\").reset_index(name=\"sentence\").merge(df.sort_values(\"offset\").groupby([\"section\", \"time\"])[features].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(annotation_path / f\"lpp{lang}_sentence_embeddings_GloVeBag.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
